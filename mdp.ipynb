{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action dictionaries: each action represents a movement, and each has its own id\n",
    "action_dict = {\n",
    "    0: np.array([0,1]), #R\n",
    "    1: np.array([0,-1]), #L\n",
    "    2: np.array([-1,0]), #U\n",
    "    3: np.array([1,0]), #D\n",
    "}\n",
    "\n",
    "action_dict_desc = {\n",
    "    0: '>', #R\n",
    "    1: '<', #L\n",
    "    2: '^', #U\n",
    "    3: 'v', #D\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy visualization: Puts arrows correpsonding to actions given by policy pi\n",
    "def describe_policy(pi):\n",
    "    dpi = [[' ' for _ in range(pi.shape[1])] for _ in range(pi.shape[0])]\n",
    "    for i in range(pi.shape[0]):\n",
    "        for j in range(pi.shape[1]):\n",
    "            dpi[i][j] = action_dict_desc[pi[i,j]]\n",
    "            \n",
    "    return np.array(dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid dimensions\n",
    "H, W = 5, 5\n",
    "\n",
    "# Gamma: Between 0 and 1.\n",
    "#   0 - myopic agent\n",
    "#   1 - farsighted agent\n",
    "gamma = 0.9\n",
    "\n",
    "# Special states\n",
    "A = np.array((0, 1))\n",
    "A_prime = np.array((4, 1))\n",
    "\n",
    "B = np.array((0, 3))\n",
    "B_prime = np.array((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward: Given a position and an action, return the new position and reward\n",
    "def reward(pos, action):\n",
    "    new_pos = pos+action\n",
    "    \n",
    "    if new_pos[0] < 0 or new_pos[0] >= H or new_pos[1] < 0 or new_pos[1] >= W:\n",
    "        return pos, -1\n",
    "    if np.all(pos == A):\n",
    "        return A_prime, 10\n",
    "    if np.all(pos==B):\n",
    "        return B_prime, 5\n",
    "    if np.all(pos==B_prime):\n",
    "        return pos, 0\n",
    "    if np.all(pos==A_prime):\n",
    "        return pos, 0\n",
    "    \n",
    "    return new_pos, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value iteration: Performs updates on state values until optimal values v_star are obtained\n",
    "def value_iteration(V, pi):\n",
    "    theta = 0.01\n",
    "    while(True):\n",
    "        delta = 0\n",
    "        for i in range(V.shape[0]):\n",
    "            for j in range(V.shape[1]):\n",
    "                s = np.array([i,j])\n",
    "                \n",
    "                old_v = V[*s]\n",
    "                action_vals = []\n",
    "                for action_id in action_dict:\n",
    "                    action = action_dict[action_id]\n",
    "                    s_prime, r = reward(s, action)\n",
    "                    \n",
    "                    action_vals.append(r + gamma*V[*s_prime])\n",
    "                    \n",
    "                V[*s] = max(action_vals)\n",
    "                delta = max(delta, abs(old_v - V[*s]))\n",
    "                \n",
    "        if delta < theta:\n",
    "            return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a value function, for each state returns the action that maximizes the expected return\n",
    "def greedy_policy(V):\n",
    "    pi = np.zeros_like(V)\n",
    "    \n",
    "    for i in range(V.shape[0]):\n",
    "        for j in range(V.shape[1]):\n",
    "            s = np.array([i,j])\n",
    "            \n",
    "            action_vals = np.zeros(len(action_dict))\n",
    "            for action_id in action_dict:\n",
    "                action = action_dict[action_id]\n",
    "                s_prime, r = reward(s, action)\n",
    "                \n",
    "                action_vals[action_id] = r + gamma*V[*s_prime]\n",
    "                \n",
    "            pi[*s] = np.argmax(action_vals)\n",
    "            \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['>' '>' '<' '>' 'v']\n",
      " ['>' '^' '<' '<' '<']\n",
      " ['>' '^' '<' '>' '^']\n",
      " ['>' '^' '<' '<' '<']\n",
      " ['^' '>' '^' '<' '<']]\n"
     ]
    }
   ],
   "source": [
    "V = np.zeros((H,W))\n",
    "pi = np.random.randint(0, 4, (H,W))\n",
    "\n",
    "V_star = value_iteration(V, pi)\n",
    "pi_star = greedy_policy(V_star)\n",
    "\n",
    "pi_star_desc = describe_policy(pi_star)\n",
    "\n",
    "print(pi_star_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy evaluation: Given an initial value function and a policy, iterate the values until they converge\n",
    "# to the values corresponding to the policy\n",
    "def policy_evaluation(V, pi):\n",
    "    delta_max=0.01\n",
    "    while(True):\n",
    "        delta=0\n",
    "        for i in range(V.shape[0]):\n",
    "            for j in range(V.shape[1]):\n",
    "                val = V[i,j]\n",
    "                state = np.array([i,j])\n",
    "                action = action_dict[pi[i,j]]\n",
    "                new_state, r = reward(state, action)\n",
    "                V[i,j] = r + gamma*V[new_state[0], new_state[1]]\n",
    "                \n",
    "                delta = max(delta, np.abs(val - V[i,j]))\n",
    "\n",
    "        if delta < delta_max:\n",
    "            break\n",
    "    \n",
    "    return V \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a policy pi and its value function V, imporve the policy by taking the greedy action wrt V\n",
    "def policy_improvement(pi, V):\n",
    "    new_pi = pi.copy()\n",
    "    for i in range(V.shape[0]):\n",
    "        for j in range(V.shape[1]):\n",
    "            s = np.array([i,j])\n",
    "            a = pi[s[0],s[1]]\n",
    "            \n",
    "            max_val = -float('inf')\n",
    "            for action_id in [0,1,2,3]:\n",
    "                action = action_dict[action_id]\n",
    "                new_pos, r = reward(s, action)\n",
    "                if r + gamma*V[new_pos[0],new_pos[1]] > max_val:\n",
    "                    max_val = r + gamma*V[new_pos[0],new_pos[1]]\n",
    "                    new_pi[i,j] = action_id\n",
    "                    \n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy iteration: Combines the policy evaluation and policy imporvement steps, and iterates them until\n",
    "# the optimal policy is found\n",
    "def policy_iteration(pi, V):\n",
    "    while True:\n",
    "        V = policy_evaluation(V, pi)\n",
    "        new_pi = policy_improvement(pi, V)\n",
    "        \n",
    "        if np.all(new_pi == pi):\n",
    "            break\n",
    "        else:\n",
    "            pi = new_pi\n",
    "            \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['>' '>' '<' '>' '<']\n",
      " ['>' '^' '<' '^' '<']\n",
      " ['>' '^' '<' '>' '^']\n",
      " ['>' '^' '<' '<' '^']\n",
      " ['^' '>' '^' '<' '^']]\n"
     ]
    }
   ],
   "source": [
    "pi_star = policy_iteration(pi, V)\n",
    "pi_star_desc = describe_policy(pi_star)\n",
    "\n",
    "print(pi_star_desc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
